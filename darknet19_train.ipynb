{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'chainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1c12adf8a084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# from darknet19 import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/tsuchiyashouto/Documents/9_Python_Scripts/05_Pytorch_project/yolo_v2_pytorch/lib/image_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# src_imageの背景画像に対して、overlay_imageのalpha画像を貼り付ける。pos_xとpos_yは貼り付け時の左上の座標\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tsuchiyashouto/Documents/9_Python_Scripts/05_Pytorch_project/yolo_v2_pytorch/lib/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'chainer'"
     ]
    }
   ],
   "source": [
    "# from darknet19 import *\n",
    "from lib.image_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(1)\n",
    "b  = torch.ones(1) * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsuchiyashouto/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "from chainer import serializers, optimizers, Variable, cuda\n",
    "import chainer.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "numpy.ndarray or cuda.ndarray are expected.\nActual: <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ebcb58c9352c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/tsuchiyashouto/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/chainer/variable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m             msg = '''numpy.ndarray or cuda.ndarray are expected.\n\u001b[1;32m    459\u001b[0m Actual: {0}'''.format(type(data))\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# Use a list as a data structure to hold the data array indirectly to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: numpy.ndarray or cuda.ndarray are expected.\nActual: <class 'int'>"
     ]
    }
   ],
   "source": [
    "F.maximum(Variable(10), Variable(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "input_height, input_width = (224, 224)\n",
    "item_path = \"./items\"\n",
    "background_path = \"./backgrounds\"\n",
    "label_file = \"./data/label.txt\"\n",
    "backup_path = \"backup\"\n",
    "batch_size = 32\n",
    "max_batches = 3000\n",
    "learning_rate = 0.001\n",
    "lr_decay_power = 4\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load image generator\n",
    "print(\"loading image generator...\")\n",
    "generator = ImageGenerator(item_path, background_path)\n",
    "\n",
    "with open(label_file, \"r\") as f:\n",
    "    labels = f.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "print(\"loading model...\")\n",
    "model = Darknet19Predictor(Darknet19())\n",
    "backup_file = \"%s/backup.model\" % (backup_path)\n",
    "if os.path.isfile(backup_file):\n",
    "    serializers.load_hdf5(backup_file, model) # load saved model\n",
    "model.predictor.train = True\n",
    "cuda.get_device(0).use()\n",
    "model.to_gpu() # for gpu\n",
    "\n",
    "optimizer = optimizers.MomentumSGD(lr=learning_rate, momentum=momentum)\n",
    "optimizer.use_cleargrads()\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(weight_decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "print(\"loading model...\")\n",
    "model = Darknet19Predictor(Darknet19())\n",
    "backup_file = \"%s/backup.model\" % (backup_path)\n",
    "if os.path.isfile(backup_file):\n",
    "    serializers.load_hdf5(backup_file, model) # load saved model\n",
    "model.predictor.train = True\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start to train\n",
    "print(\"start training\")\n",
    "for batch in range(max_batches):\n",
    "    # generate sample\n",
    "    x, t = generator.generate_samples(\n",
    "        n_samples=batch_size,\n",
    "        n_items=1,\n",
    "        crop_width=input_width,\n",
    "        crop_height=input_height,\n",
    "        min_item_scale=0.3,\n",
    "        max_item_scale=1.3,\n",
    "        rand_angle=25,\n",
    "        minimum_crop=0.8,\n",
    "        delta_hue=0.01,\n",
    "        delta_sat_scale=0.5,\n",
    "        delta_val_scale=0.5\n",
    "    )\n",
    "    x = Variable(x)\n",
    "    one_hot_t = []\n",
    "    for i in range(len(t)):\n",
    "        one_hot_t.append(t[i][0][\"one_hot_label\"])\n",
    "    x.to_gpu()\n",
    "    one_hot_t = np.array(one_hot_t, dtype=np.float32)\n",
    "    one_hot_t = Variable(one_hot_t)\n",
    "    one_hot_t.to_gpu()\n",
    "\n",
    "    y, loss, accuracy = model(x, one_hot_t)\n",
    "    print(\"[batch %d (%d images)] learning rate: %f, loss: %f, accuracy: %f\" % (batch+1, (batch+1) * batch_size, optimizer.lr, loss.data, accuracy.data))\n",
    "\n",
    "    optimizer.zero_grads()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.lr = learning_rate * (1 - batch / max_batches) ** lr_decay_power # Polynomial decay learning rate\n",
    "    optimizer.update()\n",
    "\n",
    "    # save model\n",
    "    if (batch+1) % 1000 == 0:\n",
    "        model_file = \"%s/%s.model\" % (backup_path, batch+1)\n",
    "        print(\"saving model to %s\" % (model_file))\n",
    "        serializers.save_hdf5(model_file, model)\n",
    "        serializers.save_hdf5(backup_file, model)\n",
    "\n",
    "print(\"saving model to %s/darknet19_final.model\" % (backup_path))\n",
    "serializers.save_hdf5(\"%s/darknet19_final.model\" % (backup_path), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start to train\n",
    "print(\"start training\")\n",
    "for batch in range(max_batches):\n",
    "    # generate sample\n",
    "    x, t = generator.generate_samples(\n",
    "        n_samples=batch_size,\n",
    "        n_items=1,\n",
    "        crop_width=input_width,\n",
    "        crop_height=input_height,\n",
    "        min_item_scale=0.3,\n",
    "        max_item_scale=1.3,\n",
    "        rand_angle=25,\n",
    "        minimum_crop=0.8,\n",
    "        delta_hue=0.01,\n",
    "        delta_sat_scale=0.5,\n",
    "        delta_val_scale=0.5\n",
    "    )\n",
    "    x = Variable(x)\n",
    "    x.cuda()\n",
    "    one_hot_t = []\n",
    "    for i in range(len(t)):\n",
    "        one_hot_t.append(t[i][0][\"one_hot_label\"])\n",
    "    one_hot_t = np.array(one_hot_t, dtype=np.float32)\n",
    "    one_hot_t = Variable(one_hot_t)\n",
    "    one_hot_t.cuda()\n",
    "    \n",
    "    optimizer.zero_grads()\n",
    "    y = model(x)\n",
    "    loss = F.cross_entropy(y, one_hot_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"[batch %d (%d images)] learning rate: %f, loss: %f, accuracy: %f\" % (batch+1, (batch+1) * batch_size, optimizer.lr, loss.data, accuracy.data))\n",
    "\n",
    "    optimizer.lr = learning_rate * (1 - batch / max_batches) ** lr_decay_power # Polynomial decay learning rate\n",
    "    optimizer.update()\n",
    "\n",
    "    # save model\n",
    "    if (batch+1) % 1000 == 0:\n",
    "        model_file = \"%s/%s.model\" % (backup_path, batch+1)\n",
    "        print(\"saving model to %s\" % (model_file))\n",
    "        serializers.save_hdf5(model_file, model)\n",
    "        serializers.save_hdf5(backup_file, model)\n",
    "\n",
    "print(\"saving model to %s/darknet19_final.model\" % (backup_path))\n",
    "serializers.save_hdf5(\"%s/darknet19_final.model\" % (backup_path), model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-4.2.0]",
   "language": "python",
   "name": "conda-env-anaconda3-4.2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
