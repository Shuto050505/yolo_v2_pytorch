{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.network as net_utils\n",
    "import cfgs.config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anchors = np.asarray([(1.08, 1.19), (3.42, 4.41), (6.63, 11.38), (9.42, 5.11), (16.62, 10.52)], dtype=np.float)\n",
    "num_anchors = len(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorg_function(x, stride=2):\n",
    "    batch_size, input_channel, input_height, input_width = x.size()\n",
    "    output_height, output_width, output_channel = int(input_height/stride), int(input_width/stride), input_channel*stride*stride\n",
    "    output = x.view(batch_size, input_channel, output_height, stride, output_width, stride).permute(0, 1, 2, 4, 3, 5).contiguous()\n",
    "    output = output.view(batch_size, input_channel, output_height, output_width, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "    output = output.view(batch_size, output_channel, output_height, output_width).contiguous()\n",
    "    return output\n",
    "\n",
    "class ReorgLayer(nn.Module):\n",
    "    def __init__(self, stride):\n",
    "        super(ReorgLayer, self).__init__()\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = reorg_function(x, self.stride)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conv2d_BatchNorm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 relu=True, same_padding=False):\n",
    "        super(Conv2d_BatchNorm, self).__init__()\n",
    "        padding = int((kernel_size - 1) / 2) if same_padding else 0\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                              stride, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, momentum=0.01)\n",
    "        self.relu = nn.LeakyReLU(0.1, inplace=True) if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _make_layers(in_channels, net_cfg):\n",
    "    layers = []\n",
    "\n",
    "    if len(net_cfg) > 0 and isinstance(net_cfg[0], list):\n",
    "        for sub_cfg in net_cfg:\n",
    "            layer, in_channels = _make_layers(in_channels, sub_cfg)\n",
    "            layers.append(layer)\n",
    "    else:\n",
    "        for item in net_cfg:\n",
    "            if item == 'M':\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            else:\n",
    "                out_channels, ksize = item\n",
    "                layers.append(net_utils.Conv2d_BatchNorm(in_channels,\n",
    "                                                         out_channels,\n",
    "                                                         ksize,\n",
    "                                                         same_padding=True))\n",
    "                # layers.append(net_utils.Conv2d(in_channels, out_channels,\n",
    "                #     ksize, same_padding=True))\n",
    "                in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(*layers), in_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Darknet19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Darknet19, self).__init__()\n",
    "\n",
    "        net_cfgs = [\n",
    "            # conv1s\n",
    "            [(32, 3)],\n",
    "            ['M', (64, 3)],\n",
    "            ['M', (128, 3), (64, 1), (128, 3)],\n",
    "            ['M', (256, 3), (128, 1), (256, 3)],\n",
    "            ['M', (512, 3), (256, 1), (512, 3), (256, 1), (512, 3)],\n",
    "            # conv2\n",
    "            ['M', (1024, 3), (512, 1), (1024, 3), (512, 1), (1024, 3)],\n",
    "            # ------------\n",
    "            # conv3\n",
    "            [(1024, 3), (1024, 3)],\n",
    "            # conv4\n",
    "            [(1024, 3)]\n",
    "        ]\n",
    "\n",
    "        # darknet\n",
    "        self.conv1s, c1 = _make_layers(3, net_cfgs[0:5])\n",
    "        self.conv2, c2 = _make_layers(c1, net_cfgs[5])\n",
    "        # ---\n",
    "        self.conv3, c3 = _make_layers(c2, net_cfgs[6])\n",
    "\n",
    "        stride = 2\n",
    "        # stride*stride times the channels of conv1s\n",
    "        self.reorg = ReorgLayer(stride=2)\n",
    "        # cat [conv1s, conv3]\n",
    "        self.conv4, c4 = _make_layers((c1*(stride*stride) + c3), net_cfgs[7])\n",
    "\n",
    "        # linear\n",
    "        out_channels = num_anchors * (num_classes + 5)\n",
    "        self.conv5 = net_utils.Conv2d(c4, out_channels, 1, 1, relu=False)\n",
    "        self.global_average_pool = nn.AvgPool2d((1, 1))\n",
    "\n",
    "        # train\n",
    "        self.bbox_loss = None\n",
    "        self.iou_loss = None\n",
    "        self.cls_loss = None\n",
    "        self.pool = Pool(processes=10)\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self.bbox_loss + self.iou_loss + self.cls_loss\n",
    "\n",
    "    def forward(self, im_data, gt_boxes=None, gt_classes=None, dontcare=None,\n",
    "                size_index=0):\n",
    "        conv1s = self.conv1s(im_data)\n",
    "        conv2 = self.conv2(conv1s)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv1s_reorg = self.reorg(conv1s)\n",
    "        cat_1_3 = torch.cat([conv1s_reorg, conv3], 1)\n",
    "        conv4 = self.conv4(cat_1_3)\n",
    "        conv5 = self.conv5(conv4)   # batch_size, out_channels, h, w\n",
    "        global_average_pool = self.global_average_pool(conv5)\n",
    "\n",
    "        # for detection\n",
    "        # bsize, c, h, w -> bsize, h, w, c ->\n",
    "        #                   bsize, h x w, num_anchors, 5+num_classes\n",
    "        bsize, _, h, w = global_average_pool.size()\n",
    "        # assert bsize == 1, 'detection only support one image per batch'\n",
    "        global_average_pool_reshaped = global_average_pool.permute(0, 2, 3, 1).contiguous().view(bsize, -1, num_anchors, num_classes + 5)  # noqa\n",
    "\n",
    "        # tx, ty, tw, th, to -> sig(tx), sig(ty), exp(tw), exp(th), sig(to)\n",
    "        xy_pred = F.sigmoid(global_average_pool_reshaped[:, :, :, 0:2])\n",
    "        wh_pred = torch.exp(global_average_pool_reshaped[:, :, :, 2:4])\n",
    "        bbox_pred = torch.cat([xy_pred, wh_pred], 3)\n",
    "        iou_pred = F.sigmoid(global_average_pool_reshaped[:, :, :, 4:5])\n",
    "\n",
    "        score_pred = global_average_pool_reshaped[:, :, :, 5:].contiguous()\n",
    "        prob_pred = F.softmax(score_pred.view(-1, score_pred.size()[-1])).view_as(score_pred)  # noqa\n",
    "\n",
    "        return bbox_pred, iou_pred, prob_pred\n",
    "    \n",
    "    \n",
    "    def _build_target(self, bbox_pred_np, gt_boxes, gt_classes, dontcare,\n",
    "                      iou_pred_np, size_index):\n",
    "        \"\"\"\n",
    "        :param bbox_pred: shape: (bsize, h x w, num_anchors, 4) :\n",
    "                          (sig(tx), sig(ty), exp(tw), exp(th))\n",
    "        \"\"\"\n",
    "\n",
    "        bsize = bbox_pred_np.shape[0]\n",
    "\n",
    "        targets = self.pool.map(partial(_process_batch, size_index=size_index),\n",
    "                                ((bbox_pred_np[b], gt_boxes[b],\n",
    "                                  gt_classes[b], dontcare[b], iou_pred_np[b])\n",
    "                                 for b in range(bsize)))\n",
    "\n",
    "        _boxes = np.stack(tuple((row[0] for row in targets)))\n",
    "        _ious = np.stack(tuple((row[1] for row in targets)))\n",
    "        _classes = np.stack(tuple((row[2] for row in targets)))\n",
    "        _box_mask = np.stack(tuple((row[3] for row in targets)))\n",
    "        _iou_mask = np.stack(tuple((row[4] for row in targets)))\n",
    "        _class_mask = np.stack(tuple((row[5] for row in targets)))\n",
    "\n",
    "        return _boxes, _ious, _classes, _box_mask, _iou_mask, _class_mask\n",
    "\n",
    "    \n",
    "    def load_from_npz(self, fname, num_conv=None):\n",
    "        dest_src = {'conv.weight': 'kernel', 'conv.bias': 'biases',\n",
    "                    'bn.weight': 'gamma', 'bn.bias': 'biases',\n",
    "                    'bn.running_mean': 'moving_mean',\n",
    "                    'bn.running_var': 'moving_variance'}\n",
    "        params = np.load(fname)\n",
    "        own_dict = self.state_dict()\n",
    "        keys = list(own_dict.keys())\n",
    "\n",
    "        for i, start in enumerate(range(0, len(keys), 5)):\n",
    "            if num_conv is not None and i >= num_conv:\n",
    "                break\n",
    "            end = min(start+5, len(keys))\n",
    "            for key in keys[start:end]:\n",
    "                list_key = key.split('.')\n",
    "                ptype = dest_src['{}.{}'.format(list_key[-2], list_key[-1])]\n",
    "                src_key = '{}-convolutional/{}:0'.format(i, ptype)\n",
    "                print((src_key, own_dict[key].size(), params[src_key].shape))\n",
    "                param = torch.from_numpy(params[src_key])\n",
    "                if ptype == 'kernel':\n",
    "                    param = param.permute(3, 2, 0, 1)\n",
    "                own_dict[key].copy_(param)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Darknet19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-4.2.0]",
   "language": "python",
   "name": "conda-env-anaconda3-4.2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
