{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from darknet19 import *\n",
    "from lib.image_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import chainer\n",
    "# from chainer import serializers, optimizers, Variable, cuda\n",
    "# import chainer.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "input_height, input_width = (224, 224)\n",
    "item_path = \"./items\"\n",
    "background_path = \"./backgrounds\"\n",
    "label_file = \"./data/label.txt\"\n",
    "backup_path = \"backup\"\n",
    "batch_size = 32\n",
    "max_batches = 3000\n",
    "learning_rate = 0.001\n",
    "lr_decay_power = 4\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load image generator\n",
    "print(\"loading image generator...\")\n",
    "generator = ImageGenerator(item_path, background_path)\n",
    "\n",
    "with open(label_file, \"r\") as f:\n",
    "    labels = f.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "print(\"loading model...\")\n",
    "model = Darknet19Predictor(Darknet19())\n",
    "backup_file = \"%s/backup.model\" % (backup_path)\n",
    "if os.path.isfile(backup_file):\n",
    "    serializers.load_hdf5(backup_file, model) # load saved model\n",
    "model.predictor.train = True\n",
    "cuda.get_device(0).use()\n",
    "model.to_gpu() # for gpu\n",
    "\n",
    "optimizer = optimizers.MomentumSGD(lr=learning_rate, momentum=momentum)\n",
    "optimizer.use_cleargrads()\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(weight_decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "print(\"loading model...\")\n",
    "model = Darknet19Predictor(Darknet19())\n",
    "backup_file = \"%s/backup.model\" % (backup_path)\n",
    "if os.path.isfile(backup_file):\n",
    "    serializers.load_hdf5(backup_file, model) # load saved model\n",
    "model.predictor.train = True\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start to train\n",
    "print(\"start training\")\n",
    "for batch in range(max_batches):\n",
    "    # generate sample\n",
    "    x, t = generator.generate_samples(\n",
    "        n_samples=batch_size,\n",
    "        n_items=1,\n",
    "        crop_width=input_width,\n",
    "        crop_height=input_height,\n",
    "        min_item_scale=0.3,\n",
    "        max_item_scale=1.3,\n",
    "        rand_angle=25,\n",
    "        minimum_crop=0.8,\n",
    "        delta_hue=0.01,\n",
    "        delta_sat_scale=0.5,\n",
    "        delta_val_scale=0.5\n",
    "    )\n",
    "    x = Variable(x)\n",
    "    one_hot_t = []\n",
    "    for i in range(len(t)):\n",
    "        one_hot_t.append(t[i][0][\"one_hot_label\"])\n",
    "    x.to_gpu()\n",
    "    one_hot_t = np.array(one_hot_t, dtype=np.float32)\n",
    "    one_hot_t = Variable(one_hot_t)\n",
    "    one_hot_t.to_gpu()\n",
    "\n",
    "    y, loss, accuracy = model(x, one_hot_t)\n",
    "    print(\"[batch %d (%d images)] learning rate: %f, loss: %f, accuracy: %f\" % (batch+1, (batch+1) * batch_size, optimizer.lr, loss.data, accuracy.data))\n",
    "\n",
    "    optimizer.zero_grads()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.lr = learning_rate * (1 - batch / max_batches) ** lr_decay_power # Polynomial decay learning rate\n",
    "    optimizer.update()\n",
    "\n",
    "    # save model\n",
    "    if (batch+1) % 1000 == 0:\n",
    "        model_file = \"%s/%s.model\" % (backup_path, batch+1)\n",
    "        print(\"saving model to %s\" % (model_file))\n",
    "        serializers.save_hdf5(model_file, model)\n",
    "        serializers.save_hdf5(backup_file, model)\n",
    "\n",
    "print(\"saving model to %s/darknet19_final.model\" % (backup_path))\n",
    "serializers.save_hdf5(\"%s/darknet19_final.model\" % (backup_path), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start to train\n",
    "print(\"start training\")\n",
    "for batch in range(max_batches):\n",
    "    # generate sample\n",
    "    x, t = generator.generate_samples(\n",
    "        n_samples=batch_size,\n",
    "        n_items=1,\n",
    "        crop_width=input_width,\n",
    "        crop_height=input_height,\n",
    "        min_item_scale=0.3,\n",
    "        max_item_scale=1.3,\n",
    "        rand_angle=25,\n",
    "        minimum_crop=0.8,\n",
    "        delta_hue=0.01,\n",
    "        delta_sat_scale=0.5,\n",
    "        delta_val_scale=0.5\n",
    "    )\n",
    "    x = Variable(x)\n",
    "    x.cuda()\n",
    "    one_hot_t = []\n",
    "    for i in range(len(t)):\n",
    "        one_hot_t.append(t[i][0][\"one_hot_label\"])\n",
    "    one_hot_t = np.array(one_hot_t, dtype=np.float32)\n",
    "    one_hot_t = Variable(one_hot_t)\n",
    "    one_hot_t.cuda()\n",
    "    \n",
    "    optimizer.zero_grads()\n",
    "    y = model(x)\n",
    "    loss = F.cross_entropy(y, one_hot_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"[batch %d (%d images)] learning rate: %f, loss: %f, accuracy: %f\" % (batch+1, (batch+1) * batch_size, optimizer.lr, loss.data, accuracy.data))\n",
    "\n",
    "    optimizer.lr = learning_rate * (1 - batch / max_batches) ** lr_decay_power # Polynomial decay learning rate\n",
    "    optimizer.update()\n",
    "\n",
    "    # save model\n",
    "    if (batch+1) % 1000 == 0:\n",
    "        model_file = \"%s/%s.model\" % (backup_path, batch+1)\n",
    "        print(\"saving model to %s\" % (model_file))\n",
    "        serializers.save_hdf5(model_file, model)\n",
    "        serializers.save_hdf5(backup_file, model)\n",
    "\n",
    "print(\"saving model to %s/darknet19_final.model\" % (backup_path))\n",
    "serializers.save_hdf5(\"%s/darknet19_final.model\" % (backup_path), model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-4.2.0]",
   "language": "python",
   "name": "conda-env-anaconda3-4.2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
